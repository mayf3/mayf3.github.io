<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Home</title>
    <link>http://mayf3.github.io/</link>
    <description>Recent content on My Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 25 Oct 2020 09:28:00 +0800</lastBuildDate>
    
        <atom:link href="http://mayf3.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>简介knn算法</title>
      <link>http://mayf3.github.io/post/knn/</link>
      <pubDate>Sun, 25 Oct 2020 09:28:00 +0800</pubDate>
      
      <guid>http://mayf3.github.io/post/knn/</guid>
      
        <description>&lt;h2 id=&#34;什么是knn&#34;&gt;什么是knn&lt;/h2&gt;
&lt;p&gt;knn是Cover和Hart在1968年的&lt;a href=&#34;https://pdfs.semanticscholar.org/a3c7/50febe8e72a1e377fbae1a723768b233e9e9.pdf?_ga=2.260660749.1635069309.1608960216-652721388.1605965658&#34;&gt;论文&lt;/a&gt;里提出的分类算法，全名叫K Nearest Neighbor，中文翻译是k近邻算法。&lt;br&gt;
knn的思想很简单，核心假设是同类别的样本在特征空间上的距离较近，但由于数据存在噪声，只找最近的样本会有问题，所以我们综合考虑最近的k个样本。&lt;br&gt;
基于这个思想，knn对每一个样本，会在训练集中找出与该样本距离最接近的k个样本，取这k个样本中出现次数最多的类别作为该样本的分类结果。&lt;/p&gt;
&lt;p&gt;用下面的例子来简单介绍一下knn，假设有10个训练数据，如下图，蓝三角形和绿圆形分别表示不同的类别：
&lt;img src=&#34;http://mayf3.github.io/knn/DEBCF616-2EB5-42C7-9CA2-0B7C2F1BC9F5.png&#34; alt=&#34;c44f2e1029eaa562953dbbd709b5846b&#34;&gt;
若我们想得到下图红点的类别，假定k为3，则我们在训练样本中找最近的3个样本，即红色圆圈内有的两个蓝三角形和一个绿圆形，于是我们将红点分类为多数类，即蓝三角。
&lt;img src=&#34;http://mayf3.github.io/knn/B4D9B651-D5A7-4A37-87A6-0AF9614DED62.png&#34; alt=&#34;c9d5a145e64e0655c315dff4cc5661a4&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;knn有哪些参数&#34;&gt;knn有哪些参数&lt;/h2&gt;
&lt;p&gt;knn的模型主要由下面三个要素决定：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;k值的选择（例子中的k=3）&lt;/li&gt;
&lt;li&gt;距离度量（例子中使用了欧式距离，所示的红色圆圈）&lt;/li&gt;
&lt;li&gt;分类决策规则（例子中以多数类别作为分类结果）&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;k值的选择&#34;&gt;k值的选择&lt;/h3&gt;
&lt;p&gt;k值的选择需要结合具体的应用数据，通常会选用较小的一个值，然后通过交叉验证法来选取最优的k值。&lt;/p&gt;
&lt;p&gt;可以从方差和偏差来考虑k值对模型的影响：&lt;strong&gt;随着k值的增大，模型的方差会逐渐变小，偏差逐渐变大。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假定k值为1，算法就退化成最近邻，此时模型对数据非常敏感，一旦数据噪声较多，则模型的准确率会很低。&lt;br&gt;
随着k值的增大，模型逐渐降低了数据噪声的干扰，但分类的结果会逐渐倾向于整体数据。&lt;br&gt;
当k值为N时，则将永远返回数据中占比最多的分类，此时方差为0。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里有一篇&lt;a href=&#34;http://scott.fortmann-roe.com/docs/BiasVariance.html&#34;&gt;文章&lt;/a&gt;，以knn为例讲解了偏差和方差的区别，可以看看。&lt;/p&gt;
&lt;h3 id=&#34;距离度量&#34;&gt;距离度量&lt;/h3&gt;
&lt;p&gt;在数学上有对于距离度量的严谨定义，这里就不详细说了。&lt;br&gt;
常见的距离度量，主要是曼哈顿距离、欧式距离（knn最常用的距离）和切比雪夫距离。&lt;br&gt;
对于维度为N的x和y向量，各个距离的公式如下:&lt;/p&gt;
&lt;p&gt;曼哈顿距离
$$
L_1 = \sum_{i=1}^N|X_i - Y_i|
$$&lt;/p&gt;
&lt;p&gt;欧式距离
$$
L_2 = \Bigg(\sum_{i=1}^N{(X_i - Y_i)^{2}}\Biggr)^{\tfrac{1}{2}}
$$&lt;/p&gt;
&lt;p&gt;切比雪夫距离
$$
L_\infty = \max_i|X_i - Y_i|
$$&lt;/p&gt;
&lt;h3 id=&#34;分类决策规则&#34;&gt;分类决策规则&lt;/h3&gt;
&lt;p&gt;knn的分类决策规则往往是多数表决，由距离最近的k个样本的多数类决定测试样本的类别。多数表决规则其实等价于经验风险最小化。&lt;br&gt;
其实还有其他方案，比如将这k个样本按照距离进行加权平均（该规则也能用来回归）等等。&lt;/p&gt;
&lt;h2 id=&#34;knn有哪些实现&#34;&gt;knn有哪些实现&lt;/h2&gt;
&lt;p&gt;knn算法的难点是如何快速在训练集里找到距离前k小的样本，最简单的实现是线性扫描，对于测试样本，计算该样本到所有训练样本的距离，排序取前k个。&lt;br&gt;
另外一种常见的实现方法是采用kd树，这里就不展开介绍了，在数据随机且维度不大的情况下，kd树的复杂度会优于线性扫描，但如果维度接近于样本数量，则kd树约等于线性扫描。&lt;/p&gt;
&lt;h2 id=&#34;knn的优缺点&#34;&gt;knn的优缺点&lt;/h2&gt;
&lt;p&gt;k近邻算法的优点如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实现简单&lt;/li&gt;
&lt;li&gt;没有显式的训练过程&lt;/li&gt;
&lt;li&gt;精度高（根据knn的论文，knn的泛化错误率不超过贝叶斯最优分类器的错误率的两倍）&lt;/li&gt;
&lt;li&gt;具有可解释性&lt;/li&gt;
&lt;li&gt;对异常值不敏感&lt;/li&gt;
&lt;li&gt;无数据输入假定&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;k近邻算法的缺点如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算复杂度高&lt;/li&gt;
&lt;li&gt;空间复杂度高&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;knn的实际应用&#34;&gt;knn的实际应用&lt;/h2&gt;
&lt;p&gt;我在&lt;a href=&#34;https://github.com/mayf3/machine_learning/tree/master/algorithm/knn&#34;&gt;gihtub仓库&lt;/a&gt;里实现了knn的线性扫描版本和kd树版本，拿mnist和鸢尾花卉数据集来评估knn的性能。&lt;br&gt;
结果发现knn的算法虽然简单，但效果确实还挺好，mnist的准确率有94%以上，鸢尾花卉的准确率也有93%(42/45)。&lt;br&gt;
此外我也在鸢尾花卉数据集上尝试对特征进行归一化，用min-max归一化算法后准确率达到95%(43/45)，用z-score归一化后准确率达到98%(44/45)。&lt;/p&gt;
&lt;p&gt;但knn有一个显著的问题，就是运行效率低，特别是在mnist数据集上，1w的测试数据，6w的训练数据，特征数量有28^2=784个。&lt;br&gt;
如果用线性扫描法，则总的运算量达到1w * 6w * 784，假设计算机每秒运算量为10^8，也需要4707秒，超过一个小时。&lt;br&gt;
尝试用kd树来优化knn，结果其性能反而比线性扫描还差。&lt;/p&gt;
&lt;p&gt;如果想针对knn的效率问题进行优化，之后可以尝试一下非精确的knn算法，或则是对数据进行降维。&lt;/p&gt;
&lt;h2 id=&#34;knn的常见问题&#34;&gt;knn的常见问题&lt;/h2&gt;
&lt;h3 id=&#34;归一化&#34;&gt;归一化&lt;/h3&gt;
&lt;p&gt;knn使用的是欧式距离，欧式距离对数据的尺度很敏感，比如下面的例子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假定身高的单位为米，体重的单位为克，很明显这两者在尺度上就有巨大的差异。&lt;br&gt;
此时一个1.7m，60kg的样本和1.0m，60kg的样本的距离为0.7，而1.7m，60kg的样本和1.7m，59kg的样本的距离却高达1000。&lt;br&gt;
若我们是在做大人或小孩的分类，则这样的距离计算会很影响knn的性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在大多数情况下，我们都不希望由于特征的尺度不同，而导致其对距离的贡献不一，比如上面例子中，身高的特征即使差异很大，也几乎不影响距离的计算。
为了解决这种问题，我们在使用knn的时候（或则其他和距离尺度相关的算法），我们常常会使用下面的归一化方法来处理数据。&lt;/p&gt;
&lt;p&gt;min-max归一化，需要对每一个特征计算min和max。
$$
m = \frac{(x - x_{min})}{(x_{max} - x_{min})}
$$&lt;/p&gt;
&lt;p&gt;z-score归一化，需要对每一个特征计算均值和标准差。
$$
z = (x - \mu) / \sigma
$$&lt;/p&gt;
&lt;p&gt;当我在尝试knn算法的时候，也很好奇这两种归一化算法的优劣，然后就搜到了这篇&lt;a href=&#34;http://j.mecs-press.net/ijcnis/ijcnis-v9-n11/IJCNIS-V9-N11-4.pdf&#34;&gt;论文&lt;/a&gt;。&lt;br&gt;
从论文里给出的对比效果来看，似乎min-max的要比z-score略好一点：
&lt;img src=&#34;http://mayf3.github.io/knn/B4491EA5-817E-4F13-8994-50BE70C206D2.png&#34; alt=&#34;427ee4485988d610a448fa4f96101bfd&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;机器学习实战 第二章 k-近邻算法&lt;br&gt;
机器学习 第十章 降维与度量学习&lt;br&gt;
统计学习方法 第三章 k近邻法&lt;br&gt;
特征工程入门与实践 第三章 特征增强：清洗数据 - 3.3 标准化和归一化&lt;br&gt;
&lt;a href=&#34;https://pdfs.semanticscholar.org/a3c7/50febe8e72a1e377fbae1a723768b233e9e9.pdf?_ga=2.260660749.1635069309.1608960216-652721388.1605965658&#34;&gt;Nearest Neighbor Pattern Classification&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;http://scott.fortmann-roe.com/docs/BiasVariance.html&#34;&gt;Understanding the Bias-Variance Tradeoff&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;http://j.mecs-press.net/ijcnis/ijcnis-v9-n11/IJCNIS-V9-N11-4.pdf&#34;&gt;Comparative Analysis of KNN Algorithm using Various Normalization Techniques&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>
